# DMEMM - Deep Maximum Entropy Markov Model for NLP

A comprehensive implementation of Maximum Entropy Markov Models (MEMM) using deep learning approaches for sentiment analysis and sequence tagging tasks. This project explores three different neural architectures for capturing contextual information in text sequences.

## Table of Contents
- [Overview](#overview)
- [Background: Maximum Entropy Markov Models](#background-maximum-entropy-markov-models)
- [Model Architectures](#model-architectures)
  - [Option 1: MLP with Random Embeddings](#option-1-mlp-with-random-embeddings)
  - [Option 2: MLP with Word2Vec Embeddings](#option-2-mlp-with-word2vec-embeddings)
  - [Option 3: BiLSTM-MEMM](#option-3-bilstm-memm)
- [Data Flow & Preprocessing](#data-flow--preprocessing)
- [Training Process](#training-process)
- [Results & Model Comparison](#results--model-comparison)
- [Installation & Usage](#installation--usage)
- [Project Structure](#project-structure)
- [Key Insights](#key-insights)

---

## Overview

This project implements three variants of deep learning models for sequence labeling, specifically targeting sentiment analysis with the following tags:
- **T-POS**: Positive sentiment
- **T-NEG**: Negative sentiment
- **T-NEU**: Neutral sentiment
- **O**: No sentiment (other)

The models combine traditional MEMM approaches with modern deep learning techniques to capture sequential dependencies and context in text data.

---

## Background: Maximum Entropy Markov Models

**MEMMs** are discriminative sequence models that predict each label conditioned on:
1. **Observations** (words/features)
2. **Previous state** (previous tag)

Unlike HMMs which model joint probability P(words, tags), MEMMs directly model conditional probability:

```
P(tag_i | word_i, tag_{i-1}, context)
```

This allows MEMMs to incorporate rich, overlapping features and avoid independence assumptions.

### MEMM Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMM Sequence Tagging                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Sentence: ["love", "this", "movie", "but", "hate", "ending"]

Step 1:          Step 2:          Step 3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "love" â”‚     â”‚  "this" â”‚     â”‚ "movie" â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”     â”‚   â”Œâ”€â”€â”€â”€â”€â”     â”‚   â”Œâ”€â”€â”€â”€â”€â”
     â””â”€â”€â†’â”‚STARTâ”‚     â””â”€â”€â†’â”‚T-POSâ”‚     â””â”€â”€â†’â”‚T-POSâ”‚
         â””â”€â”€â”¬â”€â”€â”˜         â””â”€â”€â”¬â”€â”€â”˜         â””â”€â”€â”¬â”€â”€â”˜
            â”‚               â”‚               â”‚
            â–¼               â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”
        â”‚ T-POS â”‚       â”‚ T-POS â”‚       â”‚ T-NEU â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”˜
      (Predicted)     (Predicted)     (Predicted)

Each prediction uses:
  â€¢ Current word embedding
  â€¢ Previous predicted tag
  â€¢ Context words (n-gram or LSTM)
```

---

## Model Architectures

### Option 1: MLP with Random Embeddings

**File**: `dmemm/mlp.py`

This approach learns word embeddings from scratch during training.

#### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MLP with Random Initialized Embeddings                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input: Bigram Context with Previous Tag
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  word_{i-1}     word_i       tag_{i-1}
     â”‚              â”‚              â”‚
     â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Embeddingâ”‚   â”‚Embeddingâ”‚   â”‚ One-Hot  â”‚
â”‚  Layer  â”‚   â”‚  Layer  â”‚   â”‚ Encoding â”‚
â”‚ (15-dim)â”‚   â”‚ (15-dim)â”‚   â”‚  (5-dim) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚                      â”‚
            â–¼                      â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
     â”‚ Concatenateâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚  (30 + 5)  â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ 35-dimensional vector
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Linear   â”‚
     â”‚ (35â†’128)  â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ReLU    â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Linear   â”‚
     â”‚  (128â†’4)  â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ LogSoftmaxâ”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    [T-POS, T-NEG, T-NEU, O]
    (Tag probabilities)
```

#### Key Features
- **Embedding Dimension**: 15
- **Context Size**: Bigram (previous word + current word)
- **Hidden Layer**: 128 units
- **Learns embeddings**: Embeddings are randomly initialized and trained end-to-end

#### When to Use
- Domain-specific vocabulary not in pre-trained models
- Twitter text, medical terminology, or specialized jargon
- When you have sufficient training data to learn good embeddings

---

### Option 2: MLP with Word2Vec Embeddings

**File**: `dmemm/mlp-word2vec.py`

This approach uses pre-trained Google News Word2Vec embeddings (300-dimensional).

#### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MLP with Pre-trained Word2Vec                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pre-trained Word2Vec Model (GoogleNews-vectors-negative300.bin)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                    â†“ (Frozen weights)

  word_{i-1}     word_i       tag_{i-1}
     â”‚              â”‚              â”‚
     â”‚              â”‚              â”‚
     â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word2Vecâ”‚   â”‚ Word2Vecâ”‚   â”‚ One-Hot  â”‚
â”‚  Lookup â”‚   â”‚  Lookup â”‚   â”‚ Encoding â”‚
â”‚(300-dim)â”‚   â”‚(300-dim)â”‚   â”‚  (5-dim) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚              â”‚              â”‚
     â”‚  If word not in vocab:      â”‚
     â”‚  use zero vector            â”‚
     â”‚              â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚                      â”‚
            â–¼                      â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
     â”‚ Concatenateâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚(600 + 5)   â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚ 605-dimensional vector
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Linear   â”‚
     â”‚ (605â†’300) â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ReLU    â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Linear   â”‚
     â”‚ (300â†’300) â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ReLU    â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Linear   â”‚
     â”‚  (300â†’4)  â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ LogSoftmaxâ”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    [T-POS, T-NEG, T-NEU, O]
    (Tag probabilities)
```

#### Pre-trained Embeddings Handling

```
Word Vocabulary Handling
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Input Word
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word in W2V?    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚Yes    â”‚No
     â”‚       â”‚
     â–¼       â–¼
 â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ W2V â”‚  â”‚ Zero Vec â”‚
 â”‚ Vec â”‚  â”‚ (300-dim)â”‚
 â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    300-dim Vector
```

#### Key Features
- **Embedding Dimension**: 300 (pre-trained)
- **Word2Vec Model**: GoogleNews-vectors-negative300 (first 50,000 words)
- **Frozen Embeddings**: Pre-trained vectors are not updated during training
- **Out-of-vocabulary**: Words not in Word2Vec get zero vectors
- **Network Depth**: 3 fully connected layers (300 â†’ 300 â†’ 4)

#### When to Use
- **Small datasets**: Leverage knowledge from large corpora
- **General vocabulary**: Standard English words
- **Best performance**: According to results, this option performed best

---

### Option 3: BiLSTM-MEMM

**File**: `dmemm/bilstm.py`

This approach uses a Bidirectional LSTM to capture context from the entire sentence.

#### Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BiLSTM-MEMM Architecture                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input Sentence: [word_1, word_2, ..., word_n]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Sentence Encoding
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

word_1    word_2    word_3    ...    word_n
   â”‚         â”‚         â”‚              â”‚
   â–¼         â–¼         â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Embed â”‚ â”‚Embed â”‚ â”‚Embed â”‚        â”‚Embed â”‚
â”‚15-dimâ”‚ â”‚15-dimâ”‚ â”‚15-dimâ”‚   ...  â”‚15-dimâ”‚
â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜        â””â”€â”€â”¬â”€â”€â”€â”˜
   â”‚         â”‚         â”‚              â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚
        â–¼         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Bidirectional LSTM Layer      â”‚
    â”‚                                 â”‚
    â”‚  Forward â†’  â†’  â†’  â†’  â†’  â†’  â†’  â”‚
    â”‚                                 â”‚
    â”‚  â† â† â† â† â† â† â† Backward        â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚   â”‚   â”‚           â”‚
         â–¼   â–¼   â–¼           â–¼
      â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”
      â”‚ h1 â”‚â”‚ h2 â”‚  ...  â”‚ hn â”‚  (hidden states, 10-dim)
      â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜       â””â”€â”¬â”€â”€â”˜
        â”‚     â”‚            â”‚
        â–¼     â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚Linearâ”‚â”‚Linearâ”‚  â”‚Linearâ”‚
    â”‚10â†’6  â”‚â”‚10â†’6  â”‚  â”‚10â†’6  â”‚
    â””â”€â”€â”¬â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜
       â”‚      â”‚         â”‚
       â–¼      â–¼         â–¼
    [feat1][feat2]...[featn]  (features for each word)


Step 2: MEMM Scoring with Viterbi Decoding
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For each position i, compute transition scores:

    P(tag_i | features_i, tag_{i-1})


        tag_{i-1}       features_i
            â”‚               â”‚
            â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Transition Matrix  â”‚
        â”‚  + Feature Score    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
            Score(tag_i)


Viterbi Algorithm:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time:    t=0      t=1          t=2          t=3
       â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”
Tags:  â”‚STARTâ”‚  â”‚     â”‚      â”‚     â”‚      â”‚STOP â”‚
       â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”˜
          â”‚        â”‚            â”‚
          â”‚    â”Œâ”€â”€â”€â”¼â”€â”€â”€â”    â”Œâ”€â”€â”€â”¼â”€â”€â”€â”
          â”‚    â”‚   â”‚   â”‚    â”‚   â”‚   â”‚
          â–¼    â–¼   â–¼   â–¼    â–¼   â–¼   â–¼
       â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”
       â”‚T-POSâ”‚T-NEGâ”‚T-NEUâ”‚  O  â”‚T-POSâ”‚ ...
       â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜â””â”€â”¬â”€â”€â”˜
         â”‚     â”‚     â”‚     â”‚     â”‚
    Scoreâ”‚ Score â”‚ Score â”‚ Score â”‚ ...
         â”‚     â”‚     â”‚     â”‚     â”‚
         â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
          Backtrack for best path
                   â”‚
                   â–¼
       [T-POS, T-POS, T-NEU, O, T-NEG]
              (Final prediction)
```

#### BiLSTM Detailed View

```
Bidirectional LSTM Cell Processing
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

At each timestep t:

Forward Direction (â†’):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    h_{t-1}  x_t
       â”‚      â”‚
       â””â”€â”€â”¬â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ LSTM Cell â”‚
    â”‚  (forget, â”‚
    â”‚   input,  â”‚
    â”‚   output  â”‚
    â”‚   gates)  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
        h_t â†’


Backward Direction (â†):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    h_{t+1}  x_t
       â”‚      â”‚
       â””â”€â”€â”¬â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ LSTM Cell â”‚
    â”‚  (forget, â”‚
    â”‚   input,  â”‚
    â”‚   output  â”‚
    â”‚   gates)  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
        â† h_t


Combined:
â”€â”€â”€â”€â”€â”€â”€â”€
    h_t â†’ âŠ• â† h_t
         â”‚
         â–¼
    [Concatenated
     bidirectional
     hidden state]
         â”‚
         â–¼
    Feature vector
    for position t
```

#### Key Features
- **Embedding Dimension**: 15 (randomly initialized)
- **Hidden Dimension**: 10 (5 per direction)
- **Bidirectional**: Captures context from both left and right
- **Viterbi Decoding**: Finds optimal tag sequence using dynamic programming
- **Transition Matrix**: Learned conditional probabilities P(tag_i | tag_{i-1})

#### MEMM Scoring Function

```python
# For each word position, compute:
score = feature_score(word_i) + transition_score(tag_{i-1} â†’ tag_i)

# The model learns:
# 1. Feature scores from BiLSTM
# 2. Transition probabilities between tags
```

#### When to Use
- **Long-range dependencies**: Captures context from entire sentence
- **Better than n-grams**: Not limited to fixed window size
- **Structured prediction**: Viterbi ensures globally consistent tag sequences

---

## Data Flow & Preprocessing

### Data Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Processing Pipeline                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Load Raw Data
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    train_set.pkl / test_set.pkl
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ List of dicts: â”‚
    â”‚ {              â”‚
    â”‚  'words': [...],â”‚
    â”‚  'ts_raw_tags':â”‚
    â”‚         [....]  â”‚
    â”‚ }              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼

2. Create Word-Tag Tuples
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    ([words], [tags])
            â”‚
            â–¼
    Example:
    (['love', 'this', 'movie'], ['T-POS', 'T-POS', 'O'])
            â”‚
            â–¼

3. Build N-gram Contexts (Options 1 & 2)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    Bigram with Previous Tag:
    ([word_{i-1}, word_i, tag_{i-1}], tag_i)
            â”‚
            â–¼
    Example:
    (['love', 'this', 'T-POS'], 'T-POS')
    (['this', 'movie', 'T-POS'], 'O')
            â”‚
            â–¼

4. Flatten & Split
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    All bigrams from all sentences
            â”‚
            â”œâ”€â”€â†’ 80% Train
            â””â”€â”€â†’ 20% Validation
            â”‚
            â–¼

5. Convert to Tensors
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    Options 1:
    word â†’ index â†’ embedding

    Option 2:
    word â†’ Word2Vec vector (300-dim)

    Option 3:
    sentence â†’ indices â†’ embeddings â†’ BiLSTM
            â”‚
            â–¼

6. Training
â”â”â”â”â”â”â”â”â”â”â”
    Batch processing â†’ Forward pass â†’ Loss â†’ Backprop
```

### Tag Encoding

```
Tag Encoding Scheme
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Tag Name  â”‚  One-Hot Encoding    â”‚  Index
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
T-POS     â”‚  [1, 0, 0, 0, 0]    â”‚   0
T-NEG     â”‚  [0, 1, 0, 0, 0]    â”‚   1
T-NEU     â”‚  [0, 0, 1, 0, 0]    â”‚   2
O         â”‚  [0, 0, 0, 1, 0]    â”‚   3
<START>   â”‚  [0, 0, 0, 0, 1]    â”‚   4
<STOP>    â”‚  N/A                 â”‚   5
```

### Data Splits

```
Dataset Splits (80-20 split)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Original Sentences
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generate n-grams â”‚
â”‚  (or full sent.)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shuffle & Split    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
      â”‚          â”‚
      â–¼          â–¼
   Train      Validation
   (80%)        (20%)
```

---

## Training Process

### Training Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Loop (Per Epoch)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FOR each epoch:
    â”‚
    â”œâ”€â†’ FOR each training sample:
    â”‚       â”‚
    â”‚       â”œâ”€â†’ 1. Prepare Input
    â”‚       â”‚       â”œâ”€ Convert words to embeddings
    â”‚       â”‚       â”œâ”€ Encode previous tag
    â”‚       â”‚       â””â”€ Create input tensor
    â”‚       â”‚
    â”‚       â”œâ”€â†’ 2. Forward Pass
    â”‚       â”‚       â”œâ”€ Pass through network
    â”‚       â”‚       â””â”€ Get log probabilities
    â”‚       â”‚
    â”‚       â”œâ”€â†’ 3. Compute Loss
    â”‚       â”‚       â””â”€ NLL Loss between prediction and true tag
    â”‚       â”‚
    â”‚       â”œâ”€â†’ 4. Backward Pass
    â”‚       â”‚       â”œâ”€ Compute gradients
    â”‚       â”‚       â””â”€ Update parameters
    â”‚       â”‚
    â”‚       â””â”€â†’ 5. Track Loss
    â”‚
    â””â”€â†’ Return average epoch loss
```

### Loss Function

All three options use **Negative Log-Likelihood (NLL) Loss**:

```
NLL Loss
â”â”â”â”â”â”â”â”

Given:
- Predicted log probabilities: [log P(T-POS), log P(T-NEG), log P(T-NEU), log P(O)]
- True tag: T-POS (index 0)

Loss = -log P(T-POS)
     = -predicted_log_probs[0]

Goal: Minimize this loss
     â†’ Maximize probability of correct tag
```

### Optimizers

```
Optimizer Configurations
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Option 1 & 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Adam Optimizer    â”‚
â”‚  lr = 0.001        â”‚
â”‚  Î²â‚ = 0.9          â”‚
â”‚  Î²â‚‚ = 0.999        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Option 3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Adam Optimizer    â”‚
â”‚  lr = 0.01         â”‚
â”‚  (higher for LSTM) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Learning Rate Impact (from Report)

```
Learning Rate Comparison (Option 2)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

lr = 0.001 (BEST):          lr = 0.01:              lr = 0.05:
    Loss                         Loss                     Loss
     â”‚                            â”‚                         â”‚
  90 â”¤â—                        300â”¤  â—                  20000â”¤â—
     â”‚ â—                          â”‚ â— â—                      â”‚
  80 â”¤  â—                      250â”¤    â—                     â”‚
     â”‚   â—                        â”‚  â—   â—              15000â”¤
  70 â”¤    â—                    200â”¤       â—                  â”‚
     â”‚     â—                      â”‚    â—    â—                â”‚
  60 â”¤      â—â—                 150â”¤         â—           10000â”¤
     â”‚        â—                   â”‚ â—  â—      â—              â”‚
  50 â”¤         â—â—              100â”¤           â—              â”‚
     â”‚           â—                â”‚      â—      â—        5000â”¤
  40 â”¤            â—â—            50â”¤               â—â—         â”‚
     â”‚              â—              â”‚                   â—      â”‚
  30 â”¤               â—â—â—â—â—â—â—â—    0â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—â—â— 0â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     0    10   20   30 Epochs     0    10   20   30 Epochs   0    10   20   30

  Smooth convergence         Oscillating             Loss explosion
  Stable learning            Some instability         then stabilizes low
  OPTIMAL âœ“                  Acceptable               Too high âœ—
```

---

## Results & Model Comparison

### Performance Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Performance Comparison               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Model                    â”‚ Embedding    â”‚ Context    â”‚ Performance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Option 1: MLP Random     â”‚ 15-dim       â”‚ Bigram     â”‚ Moderate
                         â”‚ (learned)    â”‚ (2 words)  â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Option 2: MLP + Word2Vec â”‚ 300-dim      â”‚ Bigram     â”‚ â­ BEST
                         â”‚ (pre-trained)â”‚ (2 words)  â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Option 3: BiLSTM-MEMM    â”‚ 15-dim       â”‚ Full       â”‚ Lower
                         â”‚ (learned)    â”‚ sentence   â”‚ (tuning issues)
```

### Why Option 2 Performed Best

```
Advantages of Pre-trained Embeddings (Option 2)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Small Dataset + Pre-trained = Better Generalization
                Embeddings

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Word2Vec Model  â”‚  Trained on billions of words
â”‚  (Google News)   â”‚  from Google News corpus
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Rich semantic representations
         â”‚ ("love" â‰ˆ "enjoy", "great")
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Small Training  â”‚
â”‚  Dataset         â”‚  Only thousands of sentences
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Fine-tune classifier, not embeddings
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Better           â”‚  Leverage world knowledge
â”‚ Performance      â”‚  Less overfitting
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Random Embeddings (Option 1):
â€¢ Must learn word meanings from scratch
â€¢ Limited training data
â€¢ May overfit or underfit

BiLSTM (Option 3):
â€¢ More parameters to train
â€¢ Requires more data for optimal performance
â€¢ Complex architecture needs careful tuning
```

### Evaluation Metrics

```
Evaluation Metrics for Sequence Tagging
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Prediction vs Ground Truth:

Predicted:  [T-POS, T-POS, O,     T-NEG, O    ]
True:       [T-POS, O,     T-POS, T-NEG, O    ]
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            âœ“      âœ—      âœ—      âœ“      âœ“

Metrics Computed:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

True Positives (TP):
  Predicted sentiment tag (not O) AND correct

False Positives (FP):
  Predicted sentiment tag but was O, or wrong sentiment

False Negatives (FN):
  Predicted O but should be sentiment tag

Precision = TP / (TP + FP)
  â†’ Of predicted sentiments, how many were correct?

Recall = TP / (TP + FN)
  â†’ Of actual sentiments, how many did we find?

F1 Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
  â†’ Harmonic mean, balances precision and recall
```

---

## Installation & Usage

### Requirements

```bash
# Python 3.7+
pip install torch
pip install numpy
pip install gensim
pip install scikit-learn
pip install tqdm
pip install matplotlib
```

### Data Requirements

```
Expected Data Files:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
dmemm/
â”œâ”€â”€ train_set.pkl      # Training data (pickled)
â”œâ”€â”€ test_set.pkl       # Test data (pickled)
â””â”€â”€ GoogleNews-vectors-negative300.bin  # Word2Vec (for Option 2)

Data Format:
Each pickle file contains a list of dictionaries:
[
    {
        'words': ['word1', 'word2', ...],
        'ts_raw_tags': ['T-POS', 'O', ...]
    },
    ...
]
```

### Running the Models

#### Option 1: MLP with Random Embeddings

```bash
cd dmemm
python mlp.py
```

**Key Parameters** (edit in file):
- `EMBEDDING_DIM = 15` - Dimension of learned embeddings
- `CONTEXT_SIZE = 3` - Size of n-gram context
- `num_epochs = 15` - Number of training epochs
- `learning_rate = 0.001` - Adam optimizer learning rate

#### Option 2: MLP with Word2Vec

```bash
cd dmemm

# Download Word2Vec model first:
# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit
# Place GoogleNews-vectors-negative300.bin in dmemm/

python mlp-word2vec.py
```

**Key Parameters**:
- Uses 300-dim Word2Vec embeddings (fixed)
- Loads first 50,000 words from Word2Vec
- Out-of-vocabulary words â†’ zero vectors

#### Option 3: BiLSTM-MEMM

```bash
cd dmemm

# Training mode:
python bilstm.py --load_model 0

# Evaluation mode (load saved model):
python bilstm.py --load_model 1
```

**Key Parameters**:
- `EMBEDDING_DIM = 15` - Dimension of learned embeddings
- `HIDDEN_DIM = 10` - LSTM hidden size (5 per direction)
- `--load_model` - 0 for training, 1 to load saved model

### Model Outputs

```
Training Output:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Loss curves per epoch
â€¢ Training progress with tqdm
â€¢ Final evaluation metrics (TP, FP, FN)
â€¢ Precision, Recall, F1 Score

Example:
Epoch 1/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45678/45678 [02:34<00:00]
Loss: 85.42
...
Epoch 15/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45678/45678 [02:31<00:00]
Loss: 28.15

Evaluation:
tp, fp, fn: 1234, 567, 234
Precision: 0.685
Recall: 0.841
F1: 0.755
```

---

## Project Structure

```
dmemm/
â”‚
â”œâ”€â”€ README.md                               # This file
â”‚
â”œâ”€â”€ dmemm/
â”‚   â”œâ”€â”€ mlp.py                              # Option 1: Random embeddings
â”‚   â”œâ”€â”€ mlp-word2vec.py                     # Option 2: Word2Vec embeddings
â”‚   â”œâ”€â”€ bilstm.py                           # Option 3: BiLSTM-MEMM
â”‚   â”œâ”€â”€ report.pdf                          # Detailed experimental results
â”‚   â”‚
â”‚   â”œâ”€â”€ train_set.pkl                       # Training data (required)
â”‚   â”œâ”€â”€ test_set.pkl                        # Test data (required)
â”‚   â””â”€â”€ GoogleNews-vectors-negative300.bin  # Word2Vec model (required for Option 2)
â”‚
â””â”€â”€ saved_models/                           # (created during training)
    â””â”€â”€ hw2-bilstm.pt                       # Saved BiLSTM model
```

---

## Key Insights

### 1. Embeddings Matter for Small Datasets

```
Random vs Pre-trained Embeddings
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Small Dataset Scenario:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  Random Embeddings:                          â”‚
â”‚  â€¢ Must learn "love" means positive          â”‚
â”‚  â€¢ Needs many examples                       â”‚
â”‚  â€¢ May not generalize well                   â”‚
â”‚                                              â”‚
â”‚  Pre-trained Embeddings:                     â”‚
â”‚  â€¢ Already knows "love" â‰ˆ "enjoy" â‰ˆ "great" â”‚
â”‚  â€¢ Semantic knowledge from billions of words â”‚
â”‚  â€¢ Better generalization âœ“                   â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Context Window Trade-offs

```
Bigram (Options 1 & 2) vs Full Sentence (Option 3)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Bigram Context:
  "I [love this] movie"
        â””â”€â”¬â”€â”˜
    2-word window

  Pros: Simple, fewer parameters, faster
  Cons: Limited context

BiLSTM Context:
  "[I love this movie]"
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
   Full sentence

  Pros: Long-range dependencies, full context
  Cons: More parameters, needs more data, slower
```

### 3. MEMM Sequential Dependencies

```
Why Model Previous Tags?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Sentence: "love this movie but hate ending"

Without Previous Tag:
  love â†’ T-POS
  this â†’ ?
  movie â†’ ?
  but â†’ ?
  hate â†’ T-NEG

With Previous Tag (MEMM):
  love â†’ T-POS
  this â†’ T-POS (likely continues positive)
  movie â†’ T-POS (still in positive phrase)
  but â†’ O (transition word)
  hate â†’ T-NEG (negative)

MEMMs capture:
â€¢ Sentiment tends to span multiple words
â€¢ Transition patterns (POS â†’ POS more likely than POS â†’ NEG â†’ POS)
â€¢ Sequential structure of language
```

### 4. Viterbi Decoding (Option 3)

```
Greedy vs Viterbi
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Greedy (Options 1 & 2):
  Predict each tag independently
  â†’ May produce inconsistent sequences

Viterbi (Option 3):
  Find globally optimal sequence
  â†’ Consistent, respects transition probabilities

Example:
Greedy:  [T-POS, O, T-POS, T-NEG, O, T-POS]
         (inconsistent, jumpy)

Viterbi: [T-POS, T-POS, T-POS, O, T-NEG, T-NEG]
         (smooth transitions, more realistic)
```

### 5. Hyperparameter Tuning Importance

From the experimental results:

```
Learning Rate Impact
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Too Low (< 0.001):
  âŠ™ Slow convergence
  âŠ™ May not reach optimal

Optimal (0.001):
  âŠ™ Smooth convergence âœ“
  âŠ™ Stable training âœ“
  âŠ™ Best performance âœ“

Too High (> 0.01):
  âŠ™ Oscillating loss
  âŠ™ May miss optimal
  âŠ™ Can explode

Always tune:
â€¢ Learning rate
â€¢ Batch size
â€¢ Network architecture
â€¢ Embedding dimensions
â€¢ Number of epochs
```

---

## Future Improvements

```
Potential Enhancements
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Contextualized Embeddings
   â”œâ”€ Replace Word2Vec with BERT/RoBERTa
   â””â”€ Dynamic representations per context

2. CRF Layer (instead of MEMM)
   â”œâ”€ Conditional Random Fields
   â””â”€ Model global sequence dependencies

3. Attention Mechanisms
   â”œâ”€ Weighted context aggregation
   â””â”€ Interpretable focus on important words

4. Data Augmentation
   â”œâ”€ Synonym replacement
   â”œâ”€ Back-translation
   â””â”€ Increase training data size

5. Ensemble Methods
   â”œâ”€ Combine all three options
   â””â”€ Voting or stacking

6. Multi-task Learning
   â”œâ”€ Joint training on related tasks
   â””â”€ Transfer learning from larger datasets
```

---

## References

- **Maximum Entropy Markov Models**: McCallum et al. (2000)
- **Word2Vec**: Mikolov et al. (2013) - "Efficient Estimation of Word Representations"
- **BiLSTM for Sequence Tagging**: Graves & Schmidhuber (2005)
- **Viterbi Algorithm**: Viterbi (1967)
- **PyTorch**: https://pytorch.org/

---

## License

Academic project for CS 577 - Natural Language Processing

---

## Author

Joshua Yeung

For questions or issues, please refer to the code documentation or the detailed report.pdf.

---

## Appendix: Mathematical Formulation

### MEMM Probability

```
P(tag_sequence | word_sequence) = âˆ P(tag_i | tag_{i-1}, word_i, context_i)
                                   i=1

where each local probability is modeled by a neural network:

P(tag_i | features) = exp(NN(features)_i) / Î£ exp(NN(features)_j)
                                            j
                    = softmax(NN(features))_i
```

### BiLSTM Forward Equations

```
Forward LSTM:
â†’h_t = LSTM_forward(embedding_t, â†’h_{t-1})

Backward LSTM:
â†h_t = LSTM_backward(embedding_t, â†h_{t+1})

Combined:
h_t = [â†’h_t ; â†h_t]  (concatenation)

Features:
f_t = W Ã— h_t + b    (linear projection to tag space)
```

### Viterbi Dynamic Programming

```
Initialization:
Ï€_0(START) = 0
Ï€_0(tag) = -âˆ for tag â‰  START

Recursion:
Ï€_t(tag) = max[Ï€_{t-1}(prev_tag) + score(prev_tag â†’ tag) + feature(word_t, tag)]
           prev_tag

Backtracking:
best_tag_T = argmax Ï€_T(tag)
             tag
Trace back through saved pointers to find optimal sequence
```

---

**Happy Training!** ğŸš€
